[2024-03-15T01:43:52.875-0400] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: PII_Data_Detection.tokenize_data_task scheduled__2024-03-15T05:40:00+00:00 [queued]>
[2024-03-15T01:43:52.889-0400] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: PII_Data_Detection.tokenize_data_task scheduled__2024-03-15T05:40:00+00:00 [queued]>
[2024-03-15T01:43:52.890-0400] {taskinstance.py:2193} INFO - Starting attempt 2 of 2
[2024-03-15T01:43:52.915-0400] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): tokenize_data_task> on 2024-03-15 05:40:00+00:00
[2024-03-15T01:43:52.925-0400] {standard_task_runner.py:60} INFO - Started process 176544 to run task
[2024-03-15T01:43:52.933-0400] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'PII_Data_Detection', 'tokenize_data_task', 'scheduled__2024-03-15T05:40:00+00:00', '--job-id', '139', '--raw', '--subdir', 'DAGS_FOLDER/src/airflow.py', '--cfg-path', '/tmp/tmpo576af_6']
[2024-03-15T01:43:52.937-0400] {standard_task_runner.py:88} INFO - Job 139: Subtask tokenize_data_task
[2024-03-15T01:43:53.100-0400] {task_command.py:423} INFO - Running <TaskInstance: PII_Data_Detection.tokenize_data_task scheduled__2024-03-15T05:40:00+00:00 [running]> on host MSI.
[2024-03-15T01:43:53.405-0400] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='PII_Data_Detection' AIRFLOW_CTX_TASK_ID='tokenize_data_task' AIRFLOW_CTX_EXECUTION_DATE='2024-03-15T05:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-15T05:40:00+00:00'
[2024-03-15T01:43:53.409-0400] {logging_mixin.py:188} INFO - fetched project directory successfully /home/vineshgvk/PII-Data
[2024-03-15T01:43:53.424-0400] {logging_mixin.py:188} INFO - fetched path from resample_data task /home/vineshgvk/PII-Data/dags/processed/resampled.json
[2024-03-15T01:43:53.428-0400] {logging_mixin.py:188} INFO - fetched path from resample_data task /home/vineshgvk/PII-Data/dags/processed/label_encoder_data.json
[2024-03-15T01:43:53.428-0400] {logging_mixin.py:188} INFO - labelencoder data has been fectched successfully
[2024-03-15T01:43:53.433-0400] {logging_mixin.py:188} INFO - resampled data has been fectched successfully
[2024-03-15T01:43:54.998-0400] {logging_mixin.py:188} WARNING - /home/vineshgvk/airflow_env/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550 UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
[2024-03-15T01:43:55.812-0400] {logging_mixin.py:188} INFO - Inititated the tokenizer using the model microsoft/deberta-v3-base
[2024-03-15T01:43:55.852-0400] {logging_mixin.py:188} WARNING - Map:   0%|          | 0/20 [00:00<?, ? examples/s]
[2024-03-15T01:43:55.855-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:55.875-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:55.881-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:55.888-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:55.903-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:55.938-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:55.953-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:55.960-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:55.969-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:55.976-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:55.995-0400] {logging_mixin.py:188} WARNING - Map:  50%|#####     | 10/20 [00:00<00:00, 70.74 examples/s]
[2024-03-15T01:43:55.995-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:56.001-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:56.005-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:56.011-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:56.018-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:56.029-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:56.033-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:56.036-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:56.041-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:56.046-0400] {logging_mixin.py:188} INFO - Entered tokenize fucntion
[2024-03-15T01:43:56.091-0400] {logging_mixin.py:188} WARNING - Map: 100%|##########| 20/20 [00:00<00:00, 83.91 examples/s]
[2024-03-15T01:43:56.095-0400] {logging_mixin.py:188} WARNING - Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]
[2024-03-15T01:43:56.140-0400] {logging_mixin.py:188} WARNING - Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 22.65ba/s]
[2024-03-15T01:43:56.218-0400] {python.py:202} INFO - Done. Returned value was: Dataset({
    features: ['full_text', 'document', 'tokens', 'trailing_whitespace', 'provided_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels', 'length'],
    num_rows: 20
})
[2024-03-15T01:43:56.273-0400] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=PII_Data_Detection, task_id=tokenize_data_task, execution_date=20240315T054000, start_date=20240315T054352, end_date=20240315T054356
[2024-03-15T01:43:56.393-0400] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-03-15T01:43:56.452-0400] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
